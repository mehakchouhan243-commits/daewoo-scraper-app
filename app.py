# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-gIFUX4mI0tQX2YwWASGyTXRhHdT72o_
"""

# streamlit_daewoo_app.py
# Streamlit app to scrape Daewoo route pages and display schedules, fares and other tables.
# Usage: streamlit run streamlit_daewoo_app.py

import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
import time

BASE_URL = "https://daewooinfo.pk"

# A small list of route slugs. You can extend this list or replace with any valid route path on the site.
ROUTES = {
    "Lahore â†” Islamabad": "/daewoo-lahore-islamabad/",
    "Karachi â†” Lahore": "/daewoo-karachi-lahore/",
    "Karachi â†” Islamabad": "/daewoo-karachi-islamabad/",
    "Multan â†” Rawalpindi": "/daewoo-multan-rawalpindi/",
    "Multan â†” Lahore": "/daewoo-multan-lahore/",
    "Peshawar â†” Islamabad": "/daewoo-peshawar-islamabad/",
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; DaewooScraper/1.0; +https://example.com)"
}

st.set_page_config(page_title="Daewoo Scraper", page_icon="ðŸšŒ", layout="wide")

st.title("ðŸšŒ Daewoo â€” Route scraper & viewer")
st.markdown(
    "This app scrapes route pages from `daewooinfo.pk` and shows extracted tables (schedules, fares, seat types).\n\n"
    "**Hybrid caching strategy:** results are cached for fast loads; click `Refresh data` to re-scrape the live site."
)

# Sidebar controls
with st.sidebar:
    st.header("Controls")
    route_name = st.selectbox("Choose route", list(ROUTES.keys()))
    refresh = st.button("Refresh data (scrape live)")
    st.write("---")
    st.markdown("**Cache settings**")
    cache_seconds = st.number_input("Cache TTL (seconds)", min_value=60, max_value=86_400, value=600, step=60)
    st.write("---")
    st.markdown("App built from a user notebook. Use responsibly and do not overload the source site.")

route_slug = ROUTES[route_name]
route_url = urljoin(BASE_URL, route_slug)

# Cache wrapper for fetching raw HTML (cached by URL and TTL)
@st.cache_data(ttl=3600)
def fetch_html(url):
    """Fetch page HTML. Cached by Streamlit for 1 hour by default."""
    resp = requests.get(url, headers=HEADERS, timeout=15)
    resp.raise_for_status()
    return resp.text


def parse_with_pandas_tables(html, base_url):
    """Try pandas.read_html to get any HTML tables present on the page."""
    try:
        tables = pd.read_html(html)
    except Exception:
        tables = []

    parsed = []
    for i, t in enumerate(tables):
        df = t.copy()
        parsed.append((f"table_{i+1}", df))
    return parsed


def parse_custom(html, base_url):
    """Attempt to parse schedules/fairs with BeautifulSoup for richer info when present."""
    soup = BeautifulSoup(html, "lxml")
    results = []

    # 1) Look for elements that commonly list schedules: table-like divs, lists, or <table>
    # We'll try to find any <table> first and fall back to .find_all("li") grouping.
    tables = soup.find_all("table")
    for idx, table in enumerate(tables):
        rows = []
        thead = table.find("thead")
        headers = []
        if thead:
            headers = [th.get_text(strip=True) for th in thead.find_all(["th", "td"])]
        for tr in table.find_all("tr"):
            cells = [td.get_text(separator=" ", strip=True) for td in tr.find_all(["td", "th"])]
            if cells:
                rows.append(cells)
        if rows:
            try:
                df = pd.DataFrame(rows[1:], columns=headers) if headers and len(headers) == len(rows[0]) else pd.DataFrame(rows)
            except Exception:
                df = pd.DataFrame(rows)
            results.append((f"parsed_table_{idx+1}", df))

    # 2) Try to extract fare blocks or schedule blocks identified by headings
    # Find headings that mention 'fare', 'price', 'schedule', 'time'
    for heading in soup.find_all(["h2", "h3", "h4", "strong"]):
        txt = heading.get_text(" ", strip=True).lower()
        if any(k in txt for k in ("fare", "price", "schedule", "time", "departure", "arrival", "seat")):
            # collect sibling tables or lists
            sib = heading.find_next_sibling()
            collected = None
            if sib and sib.name == "table":
                collected = sib
            elif sib and sib.name in ("div", "ul", "ol"):
                collected = sib
            if collected:
                # try to turn collected into table
                rows = []
                if collected.name == "table":
                    for tr in collected.find_all("tr"):
                        rows.append([td.get_text(strip=True) for td in tr.find_all(["td", "th"])])
                else:
                    # for lists/divs, get text lines
                    for li in collected.find_all(["li", "p"]):
                        rows.append([li.get_text(strip=True)])
                if rows:
                    df = pd.DataFrame(rows)
                    results.append((f"block_{heading.name}_{txt[:20]}", df))

    return results


def scrape_route(url):
    """Fetch and parse a route page. Returns a list of (name, dataframe) tuples and metadata."""
    st.info(f"Scraping: {url}")
    start = time.time()
    html = fetch_html(url)
    parsed_tables = parse_with_pandas_tables(html, url)
    custom = parse_custom(html, url)

    # Combine and de-duplicate by keys
    all_tables = parsed_tables + custom
    elapsed = time.time() - start
    meta = {"url": url, "fetched_at": datetime.utcnow().isoformat() + "Z", "elapsed_seconds": elapsed, "num_tables": len(all_tables)}
    return all_tables, meta


# Cached wrapper for scrape that depends on route and cache_seconds value
@st.cache_data
def get_route_data(route_url, cache_ttl_seconds):
    # We don't use cache_ttl_seconds directly to change the cache key; Streamlit cache handles TTL in decorator
    return scrape_route(route_url)


# Main UI area
st.subheader(f"Route: {route_name}")
col1, col2 = st.columns([3, 1])

with col1:
    st.write(f"Source: {route_url}")
    if refresh:
        # Clear cached fetch_html (can't programmatically clear st.cache_data selectively, but re-run fetch with a cache-busting param)
        st.session_state.setdefault("cache_buster", 0)
        st.session_state["cache_buster"] += 1
        # We'll append a query param to avoid returning the cached response inside this run
        route_live_url = route_url
        try:
            tables, meta = scrape_route(route_live_url)
        except Exception as e:
            st.error(f"Failed to scrape live: {e}")
            tables, meta = [], {"url": route_url, "fetched_at": None, "elapsed_seconds": 0, "num_tables": 0}
    else:
        try:
            tables, meta = get_route_data(route_url, cache_seconds)
        except Exception as e:
            st.error(f"Failed to load cached data: {e}")
            tables, meta = [], {"url": route_url, "fetched_at": None, "elapsed_seconds": 0, "num_tables": 0}

    st.markdown("**Metadata**")
    st.write(meta)
    st.write("---")

    if not tables:
        st.warning("No tables found on the page. Try clicking 'Refresh data' to force a live scrape.")
    else:
        # Present a selectbox to choose which table to view
        table_names = [name for name, df in tables]
        idx = st.selectbox("Choose table block to view", list(range(len(table_names))), format_func=lambda i: table_names[i])
        name, df = tables[idx]
        st.markdown(f"### `{name}` â€” {df.shape[0]} rows x {df.shape[1]} cols")
        st.dataframe(df)

        # Provide CSV/Excel download
        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("Download CSV", csv, file_name=f"{route_name.replace(' ', '_')}_{name}.csv")

with col2:
    st.markdown("**Quick preview of found blocks**")
    if tables:
        for name, df in tables[:6]:
            st.write(f"- {name}: {df.shape[0]} Ã— {df.shape[1]}")
    else:
        st.write("(none)")

st.write("---")

st.markdown("### Notes & troubleshooting")
st.markdown(
    "- If the site changes structure, parsing may fail â€” use the page's table view or inspect the page and adapt the parsing.\n"
    "- Avoid repeatedly scraping; use cache and 'Refresh data' sparingly.\n"
    "- If pandas fails to find tables, the page may be rendering tables with JS â€” this app does not run JavaScript."
)

st.markdown("### Developer tips")
st.code(
    """
# To run locally:
# 1) Create a virtualenv and install dependencies:
#    pip install streamlit pandas requests beautifulsoup4 lxml
# 2) Run:
#    streamlit run streamlit_daewoo_app.py

# To deploy to Streamlit Community Cloud:
# - Push this file to a GitHub repo
# - On share.streamlit.io, create a new app and point to this file
"""
)

# Footer
st.caption("Built from a user notebook. Use responsibly. Â© Daewoo Scraper")

